{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nTDtnyOV2iAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alex\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from gensim.downloader import load\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v3CE9G624la1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nБудем решать задачу классификации отзывов на фильмы позитивные/негативные.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Будем решать задачу классификации отзывов на фильмы позитивные/негативные.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccptgi472iZ8",
    "outputId": "f66a52fb-530b-42f6-85ed-c2f62be397fe"
   },
   "outputs": [],
   "source": [
    "(X_train_indices, y_train), (X_test_indices, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2]),\n",
       "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjqPB9rl2kSa",
    "outputId": "477e8d24-cde8-42bd-9142-b65038aa111f"
   },
   "outputs": [],
   "source": [
    "# Получение словаря, сопоставляющего индексы со словами\n",
    "word_index = imdb.get_word_index()\n",
    "index_to_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# Функция для преобразования последовательности индексов в последовательность слов\n",
    "def indices_to_words(indices):\n",
    "    return ' '.join(index_to_word.get(index, \"\") for index in indices)\n",
    "\n",
    "# Преобразование последовательностей индексов в последовательности слов\n",
    "X_train = [indices_to_words(indices) for indices in X_train_indices]\n",
    "X_test = [indices_to_words(indices) for indices in X_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dBnyH_2L20m7"
   },
   "outputs": [],
   "source": [
    "# сокращения в отзывах\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laughter\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"LOL\": \"Laughing out loud\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don’t care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"IDC\": \"I don’t care\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"LMAO\": \"Laughing my a** off\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can’t stop laughing\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AFAIK', 'AFK', 'ASAP', 'ATK', 'ATM', 'A3', 'BAK', 'BBL', 'BBS', 'BFN', 'B4N', 'BRB', 'BRT', 'BTW', 'B4', 'CU', 'CUL8R', 'CYA', 'FAQ', 'FC', 'FWIW', 'FYI', 'GAL', 'GG', 'GN', 'GMTA', 'GR8', 'G9', 'IC', 'ICQ', 'ILU', 'IMHO', 'IMO', 'IOW', 'IRL', 'LDR', 'LMAO', 'LOL', 'LTNS', 'L8R', 'MTE', 'M8', 'NRN', 'OIC', 'PITA', 'PRT', 'PRW', 'QPSA', 'ROFL', 'ROFLOL', 'ROTFLMAO', 'SK8', 'STATS', 'ASL', 'THX', 'TTFN', 'TTYL', 'U2', 'U4E', 'WB', 'WTF', 'WTG', 'WUF', 'W8', '7K', 'TFW', 'MFW', 'MRW', 'IFYP', 'TNTL', 'JK', 'IDC', 'ILY', 'IMU', 'ADIH', 'ZZZ', 'WYWH', 'BAE', 'FIMH', 'BSAAW', 'BWL', 'BFF', 'CSL'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "froQniO_3Ggm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for text in X_train:\n",
    "    for word in text.split():\n",
    "        if word.upper() in list(chat_words.keys()):\n",
    "            i+=1\n",
    "\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'as',\n",
       " 'you',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'powerful',\n",
       " 'lets',\n",
       " 'loves',\n",
       " 'their',\n",
       " 'becomes',\n",
       " 'reaching',\n",
       " 'had',\n",
       " 'journalist',\n",
       " 'of',\n",
       " 'lot',\n",
       " 'from',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'have',\n",
       " 'after',\n",
       " 'out',\n",
       " 'atmosphere',\n",
       " 'never',\n",
       " 'more',\n",
       " 'room',\n",
       " 'and',\n",
       " 'it',\n",
       " 'so',\n",
       " 'heart',\n",
       " 'shows',\n",
       " 'to',\n",
       " 'years',\n",
       " 'of',\n",
       " 'every',\n",
       " 'never',\n",
       " 'going',\n",
       " 'and',\n",
       " 'help',\n",
       " 'moments',\n",
       " 'or',\n",
       " 'of',\n",
       " 'every',\n",
       " 'chest',\n",
       " 'visual',\n",
       " 'movie',\n",
       " 'except',\n",
       " 'her',\n",
       " 'was',\n",
       " 'several',\n",
       " 'of',\n",
       " 'enough',\n",
       " 'more',\n",
       " 'with',\n",
       " 'is',\n",
       " 'now',\n",
       " 'current',\n",
       " 'film',\n",
       " 'as',\n",
       " 'you',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'potentially',\n",
       " 'unfortunately',\n",
       " 'of',\n",
       " 'you',\n",
       " 'than',\n",
       " 'him',\n",
       " 'that',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'her',\n",
       " 'get',\n",
       " 'for',\n",
       " 'was',\n",
       " 'camp',\n",
       " 'of',\n",
       " 'you',\n",
       " 'movie',\n",
       " 'sometimes',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'with',\n",
       " 'scary',\n",
       " 'but',\n",
       " 'and',\n",
       " 'to',\n",
       " 'story',\n",
       " 'wonderful',\n",
       " 'that',\n",
       " 'in',\n",
       " 'seeing',\n",
       " 'in',\n",
       " 'character',\n",
       " 'to',\n",
       " 'of',\n",
       " '70s',\n",
       " 'musicians',\n",
       " 'with',\n",
       " 'heart',\n",
       " 'had',\n",
       " 'shadows',\n",
       " 'they',\n",
       " 'of',\n",
       " 'here',\n",
       " 'that',\n",
       " 'with',\n",
       " 'her',\n",
       " 'serious',\n",
       " 'to',\n",
       " 'have',\n",
       " 'does',\n",
       " 'when',\n",
       " 'from',\n",
       " 'why',\n",
       " 'what',\n",
       " 'have',\n",
       " 'critics',\n",
       " 'they',\n",
       " 'is',\n",
       " 'you',\n",
       " 'that',\n",
       " \"isn't\",\n",
       " 'one',\n",
       " 'will',\n",
       " 'very',\n",
       " 'to',\n",
       " 'as',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'other',\n",
       " 'and',\n",
       " 'in',\n",
       " 'of',\n",
       " 'seen',\n",
       " 'over',\n",
       " 'landed',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'of',\n",
       " 'and',\n",
       " 'br',\n",
       " \"show's\",\n",
       " 'to',\n",
       " 'whether',\n",
       " 'from',\n",
       " 'than',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'history',\n",
       " 'he',\n",
       " 'name',\n",
       " 'half',\n",
       " 'some',\n",
       " 'br',\n",
       " 'of',\n",
       " 'and',\n",
       " 'odd',\n",
       " 'was',\n",
       " 'two',\n",
       " 'most',\n",
       " 'of',\n",
       " 'mean',\n",
       " 'for',\n",
       " '1',\n",
       " 'any',\n",
       " 'an',\n",
       " 'boat',\n",
       " 'she',\n",
       " 'he',\n",
       " 'should',\n",
       " 'is',\n",
       " 'thought',\n",
       " 'frog',\n",
       " 'but',\n",
       " 'of',\n",
       " 'script',\n",
       " 'you',\n",
       " 'not',\n",
       " 'while',\n",
       " 'history',\n",
       " 'he',\n",
       " 'heart',\n",
       " 'to',\n",
       " 'real',\n",
       " 'at',\n",
       " 'barrel',\n",
       " 'but',\n",
       " 'when',\n",
       " 'from',\n",
       " 'one',\n",
       " 'bit',\n",
       " 'then',\n",
       " 'have',\n",
       " 'two',\n",
       " 'of',\n",
       " 'script',\n",
       " 'their',\n",
       " 'with',\n",
       " 'her',\n",
       " 'nobody',\n",
       " 'most',\n",
       " 'that',\n",
       " 'with',\n",
       " \"wasn't\",\n",
       " 'to',\n",
       " 'with',\n",
       " 'armed',\n",
       " 'acting',\n",
       " 'watch',\n",
       " 'an',\n",
       " 'for',\n",
       " 'with',\n",
       " 'heartfelt',\n",
       " 'film',\n",
       " 'want',\n",
       " 'an']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Попробовать в конце FastText\n",
    "2. Этапы предобработки:\n",
    "    - Замена сокращений\n",
    "    - Удаление знаков препинания *\n",
    "    - Удаление чисел *\n",
    "    - Удаление пробелов *\n",
    "    - Токенизация\n",
    "    - Лемматизация\n",
    "    \n",
    "Все, что сделаю на train надо и на test, по крайней мере - сами замены сокращений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12 ,   CSL starting'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '12 ,   CSL starting'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'12'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def remove_digit(text: str) -> str:\n",
    "    return ' '.join([word for word in text.split() if not word.isdigit()])\n",
    "\n",
    "def remove_notalpha(text: str) -> str:\n",
    "    return ' '.join([word for word in text.split() if word.isalpha()])\n",
    "\n",
    "def remove_punct(text: str) -> str:\n",
    "    return ' '.join([word for word in text.split() if word not in string.punctuation])\n",
    "    \n",
    "def remove_space(text: str) -> str:\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def replace_chat_words(text: str) -> str:\n",
    "    return \" \".join([chat_words.get(word.upper(), word).lower() for word in text.split()])\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def lemmatize(tokenize_words: list) -> str:\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokenize_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetImdb(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, lemmatizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lemmatizer = lemmatizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        text = remove_digit(text)\n",
    "        text = remove_notalpha(text)\n",
    "        text = remove_punct(text)\n",
    "        text = replace_chat_words(text)\n",
    "        text = remove_space(text)\n",
    "        words = tokenize(text)\n",
    "        text = lemmatize(words)\n",
    "        return text, label\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetImdb(X_train, y_train, tokenizer, lemmatizer)\n",
    "test_dataset = DatasetImdb(X_test, y_test, tokenizer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 25000/25000 [00:29<00:00, 838.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_prep = []\n",
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    text, _ = train_dataset[idx]  # Получаем текст без метки\n",
    "    X_train_prep.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 25000/25000 [00:27<00:00, 900.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Обработка данных в test_dataset\n",
    "X_test_prep = []\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    text, _ = test_dataset[idx]  # Получаем текст без метки\n",
    "    X_test_prep.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Count_vectorizer = CountVectorizer(max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7_jTne-12mK9"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_train_bow = vectorizer.fit_transform(X_train_prep).toarray()\n",
    "X_test_bow = vectorizer.transform(X_test_prep).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 8447)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 8447)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqrHfmgP4aZ8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Обучим небольшую нейронную сеть на задачу классификации токсичных отзывов\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "6cYiBmNU4Lae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:00<00:02,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 0.7026, accuracy_train: 0.483240008354187\n",
      "Test Loss: 0.6888, accuracy_test: 0.5388799905776978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [00:00<00:01,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 0.6883, accuracy_train: 0.5424799919128418\n",
      "Epoch: 2/10, Loss: 0.6775, accuracy_train: 0.58815997838974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [00:00<00:01,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6705, accuracy_test: 0.626479983329773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:01<00:01,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10, Loss: 0.6666, accuracy_train: 0.6412400007247925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:01<00:01,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10, Loss: 0.6556, accuracy_train: 0.7102800011634827\n",
      "Test Loss: 0.6521, accuracy_test: 0.7296800017356873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:01<00:01,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10, Loss: 0.6453, accuracy_train: 0.7557600140571594\n",
      "Epoch: 6/10, Loss: 0.6359, accuracy_train: 0.7712399959564209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:01<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6368, accuracy_test: 0.7448400259017944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:02<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10, Loss: 0.6270, accuracy_train: 0.7730799913406372\n",
      "Epoch: 8/10, Loss: 0.6185, accuracy_train: 0.7757200002670288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6225, accuracy_test: 0.7549600005149841\n",
      "Epoch: 9/10, Loss: 0.6101, accuracy_train: 0.7833999991416931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "model = SimpleNeuralNet(input_dim=9774)\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    threshold = 0.5\n",
    "    predicted = (outputs >= threshold)\n",
    "    correct = (predicted == labels)\n",
    "    accuracy = correct.sum() / len(labels)\n",
    "    return accuracy.item()\n",
    "    \n",
    "    \n",
    "epochs = 10\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "def train_neural_net(model,\n",
    "                     X_train, y_train,\n",
    "                     X_test, y_test,\n",
    "                     input_dim=9000, epochs=10,\n",
    "                     criterion=criterion,\n",
    "                     optimizer=optimizer):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "   \n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_epoch = 0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_accuracy = calculate_accuracy(pred, y_train)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}/{epochs}, Loss: {loss.item():.4f}, accuracy_train: {train_accuracy}\")\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_pred = model(X_test)\n",
    "                test_loss = criterion(test_pred, y_test)\n",
    "                test_accuracy = calculate_accuracy(test_pred, y_test)\n",
    "                print(f'Test Loss: {test_loss.item():.4f}, accuracy_test: {test_accuracy}')\n",
    "                \n",
    "        \n",
    "\n",
    "train_neural_net(model,\n",
    "                 X_train_bow, y_train,\n",
    "                 X_test_bow, y_test,\n",
    "                 input_dim=9000, epochs=10,\n",
    "                 criterion=criterion,\n",
    "                 optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LOppZ0HpBXl4",
    "outputId": "0fb51baa-df51-43ab-f01d-19e4fbd2e9b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Теперь возьмем предобученный Word2Vec, получим из него эмбеддинги слов '"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Теперь возьмем предобученный Word2Vec, получим из него эмбеддинги слов \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tVBSoLK5S9m",
    "outputId": "388ef435-0ba8-4737-d6b5-5a831f26e387"
   },
   "outputs": [],
   "source": [
    "word2vec_model = load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keh6NtpX4LdB",
    "outputId": "73cc45fa-ee9d-4d2f-fd0b-4a44745f9543"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 1.0000001192092896),\n",
       " ('Google_Nasdaq_GOOG', 0.7819362878799438),\n",
       " ('Google_GOOG', 0.7756521105766296),\n",
       " ('Google_NASDAQ_GOOG', 0.7557772397994995),\n",
       " ('Google_NSDQ_GOOG', 0.7538511753082275),\n",
       " ('Yahoo', 0.7491979598999023),\n",
       " ('GoogleGoogle', 0.7281472086906433),\n",
       " ('search_engine', 0.7255110740661621),\n",
       " ('Google_nasdaq_GOOG', 0.701485276222229),\n",
       " ('Baidu', 0.6993466019630432)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.similar_by_vector(word2vec_model['Google'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnYeyhDb9662",
    "outputId": "7468f6ed-7156-404a-c7a2-4d769c49e8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 0.5193392038345337),\n",
       " ('search_engine', 0.5044638514518738),\n",
       " ('search_engines', 0.44424811005592346),\n",
       " ('Picsearch', 0.404263973236084),\n",
       " ('Webcrawler', 0.39562055468559265),\n",
       " ('Search_Engine', 0.3874809443950653),\n",
       " ('google.com', 0.3792419731616974),\n",
       " ('AskJeeves', 0.3790159225463867),\n",
       " ('HotBot', 0.3785134255886078),\n",
       " ('MSN', 0.3783949911594391)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.similar_by_vector(word2vec_model['Google'] - word2vec_model['Apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Berlin', 0.7628204822540283),\n",
       " ('Frankfurt', 0.7316751480102539),\n",
       " ('Dusseldorf', 0.6983391046524048),\n",
       " ('Paris', 0.6756227612495422),\n",
       " ('Munich', 0.6736832857131958),\n",
       " ('Germany', 0.6483182907104492),\n",
       " ('Cologne', 0.6413757801055908),\n",
       " ('Düsseldorf', 0.6358515024185181),\n",
       " ('Stuttgart', 0.6339588165283203),\n",
       " ('Budapest', 0.6204262971878052)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.similar_by_vector(word2vec_model[\"Paris\"] - word2vec_model[\"France\"] + word2vec_model[\"Germany\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KeyedVectors.get_vector of <gensim.models.keyedvectors.KeyedVectors object at 0x000001FD14E4CA50>>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.get_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded(text: str, word2vec_model) -> list[list]:\n",
    "    embeddings_for_text = []\n",
    "    for word in text.split():\n",
    "        if word2vec_model.has_index_for(word):\n",
    "            embeddings_for_text.append(word2vec_model.get_vector(word))\n",
    "        else:\n",
    "            embeddings_for_text.append(np.zeros(word2vec_model.vector_size))\n",
    "    return np.mean(embeddings_for_text, axis=0)\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the a you with out themselves powerful let love their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart show to year of every never going and help moment or of every chest visual movie except her wa several of enough more with is now current film a you of mine potentially unfortunately of you than him that with out themselves her get for wa camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of musician with heart had shadow they of here that with her serious to have doe when from why what have critic they is you that one will very to a itself with other and in of seen over landed for anyone of and br to whether from than out themselves history he name half some br of and odd wa two most of mean for any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with to with armed acting watch an for with heartfelt film want an'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:16<00:00, 1502.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:16<00:00, 1538.28it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_emb = np.array([embedded(words, word2vec_model) for words in tqdm(X_train_prep)])\n",
    "X_test_emb = np.array([embedded(words, word2vec_model) for words in tqdm(X_test_prep)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbGJ4if3-2wN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Теперь обучим на эмбедингах от word2vec две модели: простую сеть и TextCNN\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "Y3hwEuUp-ou4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_9368\\1501977490.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:00<00:00, 26.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0/10, Loss_train: 0.6931, accuracy_train: 0.50008\n",
      "Epochs 0/10, Loss_test: 0.6929, accuracy_test: 0.53196\n",
      "Epochs 1/10, Loss_train: 0.6929, accuracy_train: 0.5378\n",
      "Epochs 2/10, Loss_train: 0.6926, accuracy_train: 0.57864\n",
      "Epochs 2/10, Loss_test: 0.6924, accuracy_test: 0.57624\n",
      "Epochs 3/10, Loss_train: 0.6924, accuracy_train: 0.58292\n",
      "Epochs 4/10, Loss_train: 0.6921, accuracy_train: 0.5706\n",
      "Epochs 4/10, Loss_test: 0.6919, accuracy_test: 0.56108\n",
      "Epochs 5/10, Loss_train: 0.6918, accuracy_train: 0.5672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 28.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 6/10, Loss_train: 0.6916, accuracy_train: 0.57108\n",
      "Epochs 6/10, Loss_test: 0.6914, accuracy_test: 0.57412\n",
      "Epochs 7/10, Loss_train: 0.6913, accuracy_train: 0.58284\n",
      "Epochs 8/10, Loss_train: 0.6910, accuracy_train: 0.59272\n",
      "Epochs 8/10, Loss_test: 0.6909, accuracy_test: 0.59264\n",
      "Epochs 9/10, Loss_train: 0.6907, accuracy_train: 0.5968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        # [25000, 300]\n",
    "        self.fc = nn.Linear(input_dim, 64)\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        out = self.fc(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "def calculate_accuracy(predicted, labels):\n",
    "    threshold = 0.5\n",
    "    predicted = predicted >= threshold\n",
    "    correct = (predicted == labels)\n",
    "    return correct.sum().item() / len(labels)\n",
    "    \n",
    "model = SimpleNeuralNet(input_dim=300)\n",
    "\n",
    "def train_neural_net(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    epochs = 10\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss_epoch = 0\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy_train = calculate_accuracy(pred, y_train)\n",
    "        print(f'Epochs {epoch}/{epochs}, Loss_train: {loss.item():.4f}, accuracy_train: {accuracy_train}')\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(X_test)\n",
    "                loss_test = criterion(pred_test, y_test)\n",
    "                accuracy_test = calculate_accuracy(pred_test, y_test)\n",
    "                print(f'Epochs {epoch}/{epochs}, Loss_test: {loss_test.item():.4f}, accuracy_test: {accuracy_test}')\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "train_neural_net(X_train_emb, y_train, X_test_emb, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(map(lambda tokens: \"</s>\" in tokens, X_train_prep[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'a',\n",
       " 'you',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'powerful',\n",
       " 'let',\n",
       " 'love',\n",
       " 'their',\n",
       " 'becomes',\n",
       " 'reaching',\n",
       " 'had',\n",
       " 'journalist',\n",
       " 'of',\n",
       " 'lot',\n",
       " 'from',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'have',\n",
       " 'after',\n",
       " 'out',\n",
       " 'atmosphere',\n",
       " 'never',\n",
       " 'more',\n",
       " 'room',\n",
       " 'and',\n",
       " 'it',\n",
       " 'so',\n",
       " 'heart',\n",
       " 'show',\n",
       " 'to',\n",
       " 'year',\n",
       " 'of',\n",
       " 'every',\n",
       " 'never',\n",
       " 'going',\n",
       " 'and',\n",
       " 'help',\n",
       " 'moment',\n",
       " 'or',\n",
       " 'of',\n",
       " 'every',\n",
       " 'chest',\n",
       " 'visual',\n",
       " 'movie',\n",
       " 'except',\n",
       " 'her',\n",
       " 'wa',\n",
       " 'several',\n",
       " 'of',\n",
       " 'enough',\n",
       " 'more',\n",
       " 'with',\n",
       " 'is',\n",
       " 'now',\n",
       " 'current',\n",
       " 'film',\n",
       " 'a',\n",
       " 'you',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'potentially',\n",
       " 'unfortunately',\n",
       " 'of',\n",
       " 'you',\n",
       " 'than',\n",
       " 'him',\n",
       " 'that',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'her',\n",
       " 'get',\n",
       " 'for',\n",
       " 'wa',\n",
       " 'camp',\n",
       " 'of',\n",
       " 'you',\n",
       " 'movie',\n",
       " 'sometimes',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'with',\n",
       " 'scary',\n",
       " 'but',\n",
       " 'and',\n",
       " 'to',\n",
       " 'story',\n",
       " 'wonderful',\n",
       " 'that',\n",
       " 'in',\n",
       " 'seeing',\n",
       " 'in',\n",
       " 'character',\n",
       " 'to',\n",
       " 'of',\n",
       " 'musician',\n",
       " 'with',\n",
       " 'heart',\n",
       " 'had',\n",
       " 'shadow',\n",
       " 'they',\n",
       " 'of',\n",
       " 'here',\n",
       " 'that',\n",
       " 'with',\n",
       " 'her',\n",
       " 'serious',\n",
       " 'to',\n",
       " 'have',\n",
       " 'doe',\n",
       " 'when',\n",
       " 'from',\n",
       " 'why',\n",
       " 'what',\n",
       " 'have',\n",
       " 'critic',\n",
       " 'they',\n",
       " 'is',\n",
       " 'you',\n",
       " 'that',\n",
       " 'one',\n",
       " 'will',\n",
       " 'very',\n",
       " 'to',\n",
       " 'a',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'other',\n",
       " 'and',\n",
       " 'in',\n",
       " 'of',\n",
       " 'seen',\n",
       " 'over',\n",
       " 'landed',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'of',\n",
       " 'and',\n",
       " 'br',\n",
       " 'to',\n",
       " 'whether',\n",
       " 'from',\n",
       " 'than',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'history',\n",
       " 'he',\n",
       " 'name',\n",
       " 'half',\n",
       " 'some',\n",
       " 'br',\n",
       " 'of',\n",
       " 'and',\n",
       " 'odd',\n",
       " 'wa',\n",
       " 'two',\n",
       " 'most',\n",
       " 'of',\n",
       " 'mean',\n",
       " 'for',\n",
       " 'any',\n",
       " 'an',\n",
       " 'boat',\n",
       " 'she',\n",
       " 'he',\n",
       " 'should',\n",
       " 'is',\n",
       " 'thought',\n",
       " 'frog',\n",
       " 'but',\n",
       " 'of',\n",
       " 'script',\n",
       " 'you',\n",
       " 'not',\n",
       " 'while',\n",
       " 'history',\n",
       " 'he',\n",
       " 'heart',\n",
       " 'to',\n",
       " 'real',\n",
       " 'at',\n",
       " 'barrel',\n",
       " 'but',\n",
       " 'when',\n",
       " 'from',\n",
       " 'one',\n",
       " 'bit',\n",
       " 'then',\n",
       " 'have',\n",
       " 'two',\n",
       " 'of',\n",
       " 'script',\n",
       " 'their',\n",
       " 'with',\n",
       " 'her',\n",
       " 'nobody',\n",
       " 'most',\n",
       " 'that',\n",
       " 'with',\n",
       " 'to',\n",
       " 'with',\n",
       " 'armed',\n",
       " 'acting',\n",
       " 'watch',\n",
       " 'an',\n",
       " 'for',\n",
       " 'with',\n",
       " 'heartfelt',\n",
       " 'film',\n",
       " 'want',\n",
       " 'an']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([0 for x in X_train_prep if 'a' in x.split()]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.0]).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Загрузка модели русского языка\n",
    "# model = api.load(\"fasttext-wiki-news-subwords-300\")  # Проверьте наличие поддерживаемых моделей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'прит' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fasttext_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mприт\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key_or_keys)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'прит' not present\""
     ]
    }
   ],
   "source": [
    "fasttext_model['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена. Количество слов: 999994\n",
      "Вектор для слова 'пример':\n",
      "[-4.210e-02 -8.830e-02  5.390e-02 -1.480e-02 -2.000e-03  1.144e-01\n",
      " -4.920e-02 -2.400e-03 -1.113e-01  5.610e-02 -2.000e-04 -3.400e-03\n",
      " -5.460e-02  6.290e-02 -1.980e-02  2.050e-02  1.450e-02 -1.950e-02\n",
      "  5.990e-02  1.690e-02 -7.770e-02  2.080e-02 -3.420e-02 -9.720e-02\n",
      "  8.260e-02  2.730e-02  1.640e-02 -1.470e-02  3.530e-02  9.220e-02\n",
      "  1.092e-01  1.210e-02  7.500e-03 -1.390e-02 -7.330e-02  1.520e-02\n",
      "  2.190e-02 -5.300e-03 -7.080e-02  1.150e-02  1.420e-02  3.370e-02\n",
      " -6.940e-02  2.100e-02  4.130e-02 -8.810e-02 -6.100e-03 -1.680e-02\n",
      " -2.410e-02 -1.360e-02 -5.100e-03 -7.370e-02 -8.415e-01  2.510e-02\n",
      " -4.320e-02  9.350e-02  2.160e-02 -4.900e-02  3.670e-02  9.940e-02\n",
      "  4.690e-02 -6.170e-02 -4.200e-02  8.680e-02  8.060e-02  1.810e-02\n",
      "  5.140e-02 -2.210e-02  4.250e-02 -7.600e-03  7.190e-02  2.490e-02\n",
      "  1.310e-02  3.150e-02  2.910e-02  4.300e-03 -1.140e-02 -7.850e-02\n",
      "  4.470e-02 -8.350e-02  1.590e-02 -7.200e-02  7.300e-03  2.190e-02\n",
      " -1.920e-02  3.920e-02 -4.490e-02  1.830e-02  4.250e-02 -4.140e-02\n",
      " -7.540e-02  3.740e-02  9.900e-03  4.930e-02 -6.410e-02 -4.820e-02\n",
      "  3.270e-02  4.890e-02 -6.210e-02  7.900e-03  3.540e-02  9.000e-04\n",
      " -1.200e-03  8.210e-02 -1.390e-02 -4.560e-02 -2.200e-03 -2.630e-02\n",
      " -5.390e-02  4.850e-02 -6.100e-03  2.010e-02  7.490e-02  3.690e-02\n",
      " -4.810e-02  2.464e-01  1.100e-03 -5.320e-02  8.690e-02  1.830e-02\n",
      " -2.750e-02  7.070e-02 -4.740e-02 -7.450e-02  6.350e-02 -4.740e-02\n",
      " -6.030e-02 -3.170e-02  9.800e-03  1.220e-02 -5.480e-02 -1.660e-02\n",
      " -3.600e-02  1.300e-02 -2.060e-02 -4.530e-02  7.180e-02 -3.540e-02\n",
      " -8.280e-02 -4.300e-02 -5.580e-02  1.220e-02  4.690e-02 -2.920e-02\n",
      " -2.400e-02 -2.160e-02 -2.161e-01  1.271e-01 -6.750e-02 -2.870e-02\n",
      " -4.130e-02  2.520e-02 -3.740e-02 -1.770e-02 -2.860e-02 -2.440e-02\n",
      " -3.860e-02 -1.573e-01  2.660e-02 -4.470e-02  1.420e-02  4.380e-02\n",
      "  3.670e-02  7.810e-02  5.360e-02 -1.660e-02 -8.200e-03 -1.099e-01\n",
      "  1.520e-02  2.690e-02 -5.000e-03  7.600e-03 -1.750e-02 -3.210e-02\n",
      " -1.437e-01  1.250e-02 -2.360e-02  3.760e-02 -4.070e-02 -5.690e-02\n",
      " -2.900e-03  1.470e-02  8.830e-02  3.560e-02  1.390e-02  1.000e-02\n",
      "  6.410e-02  1.342e-01  6.280e-02 -9.200e-03 -3.940e-02 -6.300e-03\n",
      "  1.900e-03 -7.590e-02 -6.650e-02 -4.960e-02 -6.960e-02  4.240e-02\n",
      " -8.760e-02  3.570e-02  8.700e-03 -3.140e-02  5.470e-02  9.490e-02\n",
      " -4.610e-02 -5.960e-02 -6.460e-02  3.000e-02 -5.400e-03 -2.190e-02\n",
      " -0.000e+00 -3.040e-02  5.020e-02 -2.820e-02 -4.300e-03  1.300e-01\n",
      "  1.767e-01 -2.130e-02  1.149e-01  9.500e-03  5.550e-02  2.340e-02\n",
      " -3.220e-02  6.900e-03 -3.760e-02  1.144e-01 -1.204e-01 -1.214e-01\n",
      "  1.135e-01  2.130e-02  2.490e-02 -5.080e-02  1.600e-02  1.516e-01\n",
      " -1.720e-02  5.960e-02  5.700e-02 -1.008e-01  5.160e-02 -2.490e-02\n",
      "  1.500e-03  1.600e-02 -8.400e-03 -6.550e-02  7.450e-02  8.110e-02\n",
      " -2.760e-02  1.750e-02  3.500e-03 -1.322e-01 -1.610e-02 -7.150e-02\n",
      "  1.153e-01  2.540e-02  6.600e-03 -3.180e-02  4.320e-02 -2.990e-02\n",
      " -6.670e-02  5.160e-02  1.650e-02 -5.140e-02  5.120e-02 -7.430e-02\n",
      " -2.590e-01 -1.460e-02  6.640e-02 -4.390e-02  5.280e-02  4.280e-02\n",
      " -7.490e-02 -7.510e-02  1.190e-02  7.000e-03  1.030e-02 -5.260e-02\n",
      " -2.100e-02  1.024e-01  3.530e-02  2.070e-02  4.890e-02  1.280e-02\n",
      " -2.547e-01  3.360e-02 -2.020e-02  4.740e-02  1.018e-01 -7.630e-02\n",
      " -1.110e-02 -3.090e-02 -3.960e-02  7.000e-03 -5.800e-03 -6.200e-03\n",
      " -4.790e-02 -6.590e-02 -3.950e-02  6.930e-02  2.900e-03  2.750e-02]\n",
      "Наиболее похожие слова на 'пример':\n",
      "[('качество', 0.9645180702209473), ('вже', 0.9643084406852722), ('изображения', 0.9642382264137268), ('смог', 0.9638636112213135), ('наблюдения', 0.9632663726806641)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Укажите путь к вашему файлу .vec\n",
    "vec_file_path = 'wiki-news-300d-1M.vec'\n",
    "\n",
    "# Загрузка модели\n",
    "fasttext_vectors = KeyedVectors.load_word2vec_format(vec_file_path, binary=False)\n",
    "\n",
    "# Проверка загрузки\n",
    "print(f\"Модель загружена. Количество слов: {len(fasttext_vectors.key_to_index)}\")\n",
    "\n",
    "# Получение вектора для конкретного слова\n",
    "word = 'пример'\n",
    "if word in fasttext_vectors:\n",
    "    vector = fasttext_vectors[word]\n",
    "    print(f\"Вектор для слова '{word}':\\n{vector}\")\n",
    "else:\n",
    "    print(f\"Слово '{word}' отсутствует в модели.\")\n",
    "\n",
    "# Поиск наиболее похожих слов\n",
    "similar_words = fasttext_vectors.most_similar('пример', topn=5)\n",
    "print(f\"Наиболее похожие слова на 'пример':\\n{similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trim_or_pad(vectors, pad_length, pad_vector):\n",
    "    \"\"\"\n",
    "    Обрезает или дополняет список векторов до заданной длины.\n",
    "    \"\"\"\n",
    "    assert pad_vector.ndim == 1\n",
    "    vectors = vectors[:pad_length] + [pad_vector] * max(0, pad_length - len(vectors))\n",
    "    return np.stack(vectors)\n",
    "\n",
    "def seq_to_emb(text: str, fasttext_vectors, pad_length=100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Преобразует текст в последовательность эмбеддингов FastText.\n",
    "    \n",
    "    :param text: Входной текст.\n",
    "    :param fasttext_vectors: Загруженная модель FastText.\n",
    "    :param pad_length: Длина последовательности после обрезки/дополнения.\n",
    "    :return: Массив эмбеддингов.\n",
    "    \"\"\"\n",
    "    embeddings_for_text = []\n",
    "    pad_vector = fasttext_vectors['</s>'] if '</s>' in fasttext_vectors else np.zeros(fasttext_vectors.vector_size)\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in fasttext_vectors:\n",
    "            embeddings_for_text.append(fasttext_vectors[word])\n",
    "        else:\n",
    "            # Для неизвестных слов можно использовать паддинг или средний вектор\n",
    "            embeddings_for_text.append(pad_vector)\n",
    "    \n",
    "    embedd = trim_or_pad(embeddings_for_text, pad_length, pad_vector)\n",
    "    return embedd\n",
    "\n",
    "# Пример использования\n",
    "example_text = \"Это пример текста для преобразования в эмбеддинги.\"\n",
    "embedding = seq_to_emb(example_text, fasttext_vectors, pad_length=100)\n",
    "print(embedding.shape)  # Должен вывести (100, размер_вектора)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а это для w2v оставлю\n",
    "\n",
    "def trim_or_pad(vectors, pad_length, pad_vector):\n",
    "    assert pad_vector.ndim == 1\n",
    "    vectors = vectors[:pad_length] + [pad_vector] * max(0, pad_length - len(vectors))\n",
    "    return np.stack(vectors)\n",
    "\n",
    "def seq_to_emb(text: str, word2vec_model, pad_length=100) -> list[list]:\n",
    "    embeddings_for_text = []\n",
    "    for word in text.split():\n",
    "        if word2vec_model.has_index_for(word):\n",
    "            embeddings_for_text.append(word2vec_model.get_vector(word))\n",
    "    embedd = trim_or_pad(embeddings_for_text, pad_length, word2vec_model.get_vector(\"</s>\"))\n",
    "    return embedd\n",
    "            \n",
    "    \n",
    "seq_to_emb(X_train_prep[1], word2vec_model, pad_length=100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetImdbClf(Dataset):\n",
    "    def __init__(self, texts, labels, pad_length):\n",
    "        super(DatasetImdbClf, self).__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.pad_length = pad_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        text = seq_to_emb(text, fasttext_vectors, pad_length=self.pad_length)\n",
    "        text = torch.tensor(text, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetImdbClf(X_train_prep, y_train, pad_length=100)\n",
    "test_dataset = DatasetImdbClf(X_test_prep, y_test, pad_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 300])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 300])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего 6 ядер, по 2 ядра размерами 2,3,4 каждое ядро такого размера будет проходить по 2, 3, 4 слова соответственно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "IlUk44su-YMg"
   },
   "outputs": [],
   "source": [
    "kernel_size = [2, 3, 4] # список размеров фильтров (ядер)\n",
    "num_channels = 2 # размера канала для выхода ядра\n",
    "\n",
    "class TextCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCnn, self).__init__()\n",
    "        emb_size = 300\n",
    "        self.text_filters = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Conv1d(in_channels=emb_size,\n",
    "                                    out_channels=num_channels,\n",
    "                                    kernel_size=kernel_size[i]),\n",
    "                          nn.ReLU()) for i in range(len(kernel_size))]\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=6,\n",
    "                           out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # результаты свертки с разными фильтрами\n",
    "        rv = []\n",
    "        for f in self.text_filters:\n",
    "            result = f(x.permute((0, 2, 1)))\n",
    "            # maxpooling для 3 индекса\n",
    "            result = torch.max(result, dim=2)[0]\n",
    "            rv.append(result)\n",
    "\n",
    "        x = torch.concat(rv, dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCnn()\n",
    "batch = next(iter(test_dataloader))\n",
    "\n",
    "# Разделяем входные данные и метки\n",
    "inputs, labels = batch\n",
    "\n",
    "# Передаём только входные данные в модель\n",
    "output = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCnn(\n",
       "  (text_filters): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(300, 2, kernel_size=(2,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(300, 2, kernel_size=(3,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(300, 2, kernel_size=(4,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=6, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 300])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for train_data, train_label in train_dataloader:\n",
    "    print(train_data.shape)\n",
    "    print(train_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, loss_train: 0.6921, accuracy_train: 0.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:36<05:26, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, loss_test: 0.6904, accuracy_test: 0.5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [00:56<03:33, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, loss_train: 0.6881, accuracy_train: 0.5673\n",
      "Epoch 3/10, loss_train: 0.6842, accuracy_train: 0.5767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [01:32<03:36, 30.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, loss_test: 0.6840, accuracy_test: 0.5727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [01:52<02:39, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, loss_train: 0.6801, accuracy_train: 0.5847\n",
      "Epoch 5/10, loss_train: 0.6760, accuracy_train: 0.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:28<02:30, 30.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, loss_test: 0.6777, accuracy_test: 0.5705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [02:48<01:46, 26.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, loss_train: 0.6717, accuracy_train: 0.5987\n",
      "Epoch 7/10, loss_train: 0.6675, accuracy_train: 0.6063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [03:24<01:29, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, loss_test: 0.6703, accuracy_test: 0.5979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [03:44<00:53, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, loss_train: 0.6633, accuracy_train: 0.6175\n",
      "Epoch 9/10, loss_train: 0.6590, accuracy_train: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [04:21<00:29, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, loss_test: 0.6632, accuracy_test: 0.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:42<00:00, 28.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, loss_train: 0.6543, accuracy_train: 0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = TextCnn()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "def accuracy(predicted, labels):\n",
    "    threshold = 0.5\n",
    "    predicted = predicted > 0.5\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / labels.size(0)\n",
    "\n",
    "def train_loop(model, train_dataloader, test_dataloader):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        accuracy_epoch = []\n",
    "        loss_test = []\n",
    "        accuracy_test = []\n",
    "        for train_data, train_label in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(train_data).squeeze(1)\n",
    "            loss = criterion(pred, train_label)\n",
    "            accuracy_epoch.append(accuracy(pred, train_label))\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, loss_train: {np.mean(epoch_loss):.4f}, accuracy_train: {np.mean(accuracy_epoch):.4f}\")\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for test_data, test_label in test_dataloader:\n",
    "                    pred = model(test_data).squeeze(1)\n",
    "                    loss = criterion(pred, test_label)\n",
    "                    accuracy_test.append(accuracy(pred, test_label))\n",
    "                    loss_test.append(loss.item())    \n",
    "        \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, loss_test: {np.mean(loss_test):.4f}, accuracy_test: {np.mean(accuracy_test):.4f}\")\n",
    "                \n",
    "        \n",
    "train_loop(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_text.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "04kv63LzABCY"
   },
   "outputs": [],
   "source": [
    "#Сохранить  эмбединги можно вот так\n",
    "torch.save(X_test_emb, \"X_test_emb.pt\")\n",
    "# torch.save(y_test, \"y_test_emb.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
